{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "from rfml_localization.core import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rfml_localization\n",
    "\n",
    "> Focus of repository is the location of uncooperative wireless emitters in an indoor environment.  The challenge is that an indoor environment results in dynamic, random propagation of wireless signals.  An approach is to create a RF fingerprint or map of the environment using different RF measurements techniques. Relevant measurements techniques generate a range of data from IQ samples to correlation curves to time-delay estimates (TDE's) for  TDOA; multiple power readings for RSS; and multiple angle of arrival readings for AoA.  The theoretical performance bounds are generated, via CRLB, of empirically measured and derived channel limitations that exists in literature.  From same literature, a simulation model is created that generates the specified random RF channel environment. From these foundations, regression models, that learn from the data in predicting the location of detected emitters, can be tested and validated; some of which are presented here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rfml_localization requires\n",
    "- numpy\n",
    "- sklearn\n",
    "- matplotlib\n",
    "- glmnet_py\n",
    "    - libgfortran3\n",
    "\n",
    "You can install using the following methods\n",
    "\n",
    "From source - github notebooks\n",
    "\n",
    "    git clone\n",
    "    cd rfml_localization\n",
    "    jupyter lab \n",
    "From source - github python lib\n",
    "\n",
    "    git clone\n",
    "    cd rfml_localization/rfml_localization\n",
    "    python\n",
    "    import rfml_localization.core as rfcore\n",
    "    import rfml_localization.RFsimulation as rfsim\n",
    "Eventually, pip\n",
    "\n",
    "    ~~pip install rfml_localization~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary purpose of package is to use available optimization techniques with RF fingerprinting for indoor localization. The package is written to enable the leveraging of [Sci-Kit Learn](https://scikit-learn.org/), so assumes some familiarity with said package.  In addition, it enables the use of GLMnet - (see [Glmnet Vignette](https://glmnet-python.readthedocs.io/en/latest/glmnet_vignette.html)). This package includes a simulation mode, `RFsimulation`, to generate a set of locations and associated synthetic measurements.  For optimization, the focus is on kernilzing data, see `HFF_k_matrix`, and then pairing with regression models ([Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html), [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html), [MLPregressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html), etc) within SKLearn or the Glmnet model.  The kernelized measurement parameters are tuned along with chosen model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Channel\n",
    "To showcase use, first step is create an `RFchannel` instance from the `RFsimulation` which provides methods to generate synthetic data for testing. For background on parameters, see Saleh [^1], Rappaport [^2], and Spencer [^3]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel parameters of created object: {'maxSpreadT': 100, 'PoissonInvLambda': 50, 'Poissoninvlambda': 10, 'PoissonGamma': 50, 'Poissongamma': 29, 'PathLossN': 3.0, 'Xsigma': 7, 'Wavelength': 1.4615111050485465, 'AoAsigma': 0.4537856055185257}\n"
     ]
    }
   ],
   "source": [
    "import rfml_localization.RFsimulation as rfsim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#generate channel using default channel parameters\n",
    "RFchannel_scenario1 = rfsim.RFchannel()\n",
    "#print out parameters\n",
    "print(\"Channel parameters of created object:\",vars(RFchannel_scenario1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Sensor Setup/Generate Synthetic Data\n",
    "After setting up the environment, create a sensor setup (here, fixed Rx locations) and Txmtr randomly placed at 1000 locations.  For each Txmtr location, an observation is created that contains TDoA, RSS, and AoA measurements -- see `RFsimulation.generate_Xmodel`. As methods are run on the object, additional variable are added.  This allows inspection of user-specified parameters and even methods run on the object.\n",
    "\n",
    "The generated measurements and associated locations are pulled out and then split into test, training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from channel scenario, generate locations for Tx and Rx and set of measurements\n",
    "RFchannel_scenario1.generate_RxTxlocations(n_rx=6, n_runs=1000, rxtx_flag=3)\n",
    "#generate set of measurements\n",
    "RFchannel_scenario1.generate_Xmodel()\n",
    "\n",
    "#take object's set of measurements, locations and assign to X,y\n",
    "X=RFchannel_scenario1.X_model  #measurements/observations\n",
    "y=RFchannel_scenario1.rxtx_locs[:,0,:].transpose()  #location of  Tx\n",
    "\n",
    "#split for training, test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using sklearn_kt_regressor\n",
    "After generating locations, import SKLearn models for regression.  Ridge and LASSO models were imported but any Sci-Kit Learn model should work. These models along with kernel parameters are passed to the Sci-Kit Learn-based kernel trick object, `sklearn_kt_regressor`.  `sklearn_kt_regressor` wraps the specified model and kernelized matrix into a single interface to enable use of SKLearn hyper-tuning tools.  It inherits all the basic functionality of standard Sci-Kit Learn API.  The following sets up model, sets parameters, fits the model, and then predicts.  Note that no tuning of the model's parameters has been conducted so final results are not optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Mean summed/mean physical distance error for (x,y) location estimation: 10.9 / 4.0: meters\n",
      "-----------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#based on knowledge of measurement, can derive from class instance rather than manually entering\n",
    "#variables (shape[1] of rxx_delay, rxx_rss, rxx_aoa, i.e., RFchannel_scenario1.rxx_delay.shape[1])\n",
    "num_meas_array = np.array([15,6,6]) \n",
    "\n",
    "#tuning parameter for each measurement kernel (TDoA, RSS, AoA)\n",
    "kernel_s0, kernel_s1, kernel_s2 = np.array([1.13e-06, 2.07e-03, 10])\n",
    "\n",
    "#set up the model\n",
    "skl_kt_model = sklearn_kt_regressor(skl_model = Ridge(), skl_kernel = 'rbf', \n",
    "                                    n_kernels = 3, kernel_s0 = kernel_s0, kernel_s1 = kernel_s1, \n",
    "                                    kernel_s2 = kernel_s2, n_meas_array=num_meas_array)\n",
    "\n",
    "#set model parameters - showing methods inherited from SKLearn\n",
    "skl_kt_model.set_params(skl_model__alpha = 1.83e-06)\n",
    "\n",
    "#fit the model\n",
    "skl_kt_model.fit(X_train,y_train)\n",
    "\n",
    "#predict the model\n",
    "y_pred = skl_kt_model.predict(X_test)\n",
    "\n",
    "#error measurements\n",
    "mse = mean_squared_error(y_test,y_pred)\n",
    "msec = mse_EucDistance(y_test,y_pred)\n",
    "print('-----------------------------------------------------------------------------------------------')\n",
    "print('Mean summed/mean physical distance error for (x,y) location estimation: {:3.1f} / {:3.1f}: meters'.format(mse,msec))\n",
    "print('-----------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using glmnet_kt_regressor \n",
    "Similarly, the GLMnet regressor, `glmnet_kt_regressor`, is defined in such a way to follow the Sci-Kit Learn API -- advantageous in leveraging large body of tools.  Below steps through setting up a model, fitting, and predicting.  Using same data and kernel settings as `skl_kt_regressor` example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Mean summed/mean physical distance error for (x,y) location estimation: 23.9 / 5.9: meters\n",
      "-----------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#use same training and testing set\n",
    "num_meas_array = np.array([15,6,6]) \n",
    "kernel_s0, kernel_s1, kernel_s2 = np.array([1.13e-06, 2.07e-03, 10])\n",
    "\n",
    "#set up the model, passing glmnet specific parameters via dictionary\n",
    "glmnet_args=dict(family= 'mgaussian', standardize= False)\n",
    "\n",
    "glm_kt_model = glmnet_kt_regressor(glm_alpha=0, lambdau=1e-3, skl_kernel='rbf', n_kernels=3,\n",
    "                 kernel_s0 = kernel_s0, kernel_s1 = kernel_s1, kernel_s2 = kernel_s2,\n",
    "                 n_meas_array=num_meas_array, glmnet_args=glmnet_args)\n",
    "\n",
    "#fit the model\n",
    "glm_kt_model.fit(X_train, y_train)\n",
    "\n",
    "#predict the model\n",
    "y_pred = glm_kt_model.predict(X_test)\n",
    "\n",
    "#error measurements\n",
    "mse = mean_squared_error(y_test,y_pred)\n",
    "msec = mse_EucDistance(y_test,y_pred)\n",
    "print('-----------------------------------------------------------------------------------------------')\n",
    "print('Mean summed/mean physical distance error for (x,y) location estimation: {:3.1f} / {:3.1f}: meters'.format(mse,msec))\n",
    "print('-----------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization\n",
    "\n",
    "Generally speaking, model and kernel parameters need to be tuned. Building on previous example, leverage SKLearn model tools to conduct hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using sklear_kt_regressor\n",
    "Using sklearn models.  For this example, for processing speed, n_iter is kept low.  For better performance, increase to 5k.  Note the improved performance.  Here, [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) is used for hyperparameter though any sklearn technique is viable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    6.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Mean summed/mean physical distance error for (x,y) location estimation: 23.9 / 3.5: meters\n",
      "Optimized parameters in rank order (based on tested values):\n",
      "\n",
      "  param_kernel_s0 param_kernel_s1 param_kernel_s2 param_skl_kernel  mean_test_score\n",
      "7        3.39e-06        2.71e-05        1.18e-01              rbf        -8.11e+00\n",
      "9        4.31e-06        7.34e-01        5.23e-03              rbf        -1.14e+01\n",
      "1        4.06e-07        1.71e-03        2.10e-03        laplacian        -1.91e+01\n",
      "3        1.67e+00        5.37e-03        7.96e-01              rbf        -1.94e+01\n",
      "2        1.39e-03        9.21e-04        3.03e-01              rbf        -2.11e+01\n",
      "-----------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import loguniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:.2e}'.format\n",
    "pd.options.display.width = 120\n",
    "\n",
    "#use loguniform to search uniformaly across orders of magnitude\n",
    "distributions = {\n",
    "        'skl_model__alpha': loguniform(1e-7, 1.0e+0),\n",
    "        'kernel_s0': loguniform(1e-7, 1.0e+1),\n",
    "        'kernel_s1': loguniform(1e-6, 1.0e+1), \n",
    "        'kernel_s2': loguniform(1e-4, 1.0e+2),\n",
    "        'skl_kernel': ['laplacian', 'rbf'],  # categorical parameter\n",
    "    }\n",
    "#create search model from base model\n",
    "skl_kt_model_search = RandomizedSearchCV(skl_kt_model, distributions,\n",
    "                                     scoring = 'neg_mean_squared_error', \n",
    "                                     cv = 5, n_jobs = 1, n_iter = 10, verbose=1)\n",
    "#fit search model\n",
    "skl_search_results = skl_kt_model_search.fit(X_train, y_train)\n",
    "\n",
    "#set params based on search\n",
    "skl_kt_model.set_params(**skl_search_results.best_params_)\n",
    "\n",
    "#fit model using best params ()\n",
    "skl_kt_model.fit(X_train, y_train)\n",
    "\n",
    "#predict and show error\n",
    "y_pred = skl_kt_model.predict(X_test)\n",
    "msec = mse_EucDistance(y_test,y_pred)\n",
    "skl_search_results_pd = pd.DataFrame(skl_search_results.cv_results_)\n",
    "\n",
    "print('-----------------------------------------------------------------------------------------------')\n",
    "print('Mean summed/mean physical distance error for (x,y) location estimation: {:3.1f} / {:3.1f}: meters'.format(mse,msec))\n",
    "print(\"Optimized parameters in rank order (based on tested values):\\n\")\n",
    "print(skl_search_results_pd.sort_values(by='rank_test_score').filter(items=['param_kernel_s0', 'param_kernel_s1','param_kernel_s2','param_lambdau','param_skl_kernel','mean_test_score']).head())\n",
    "print('-----------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using glmnet_kt_regressor\n",
    "Using GLMnet model. For this example, for processing speed, n_iter is kept low.  For better performance, increase to 5k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=6)]: Done  50 out of  50 | elapsed:    9.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Mean summed/mean physical distance error for (x,y) location estimation: 23.9 / 3.2: meters\n",
      "Optimized parameters in rank order (based on tested values):\n",
      "  param_kernel_s0 param_kernel_s1 param_kernel_s2 param_lambdau param_skl_kernel  mean_test_score\n",
      "7        2.56e-03        1.24e-02        2.73e-02      3.26e-07        laplacian        -6.95e+00\n",
      "0        4.83e-04        2.50e-06        1.13e-02      5.07e-04              rbf        -1.92e+01\n",
      "1        4.94e-06        7.78e-05        2.06e+00      1.11e-03              rbf        -2.44e+01\n",
      "9        2.64e-05        3.72e-01        1.32e-03      2.87e-03              rbf        -2.46e+01\n",
      "2        1.13e-07        1.84e-02        8.37e-03      1.73e-04        laplacian        -2.78e+01\n",
      "-----------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#use loguniform to search uniformally across orders of magnitude\n",
    "distributions = {\n",
    "        'lambdau': loguniform(1e-7, 1.0e+0),\n",
    "        'kernel_s0': loguniform(1e-7, 1.0e+1),\n",
    "        'kernel_s1': loguniform(1e-6, 1.0e+1), \n",
    "        'kernel_s2': loguniform(1e-4, 1.0e+2),\n",
    "        'skl_kernel': ['laplacian', 'rbf'],  # categorical parameter\n",
    "    }\n",
    "\n",
    "#create search model from base model\n",
    "#glmnet_model uses single cpu, so increase number of jobs \n",
    "glm_kt_model_search = RandomizedSearchCV(glm_kt_model, distributions,\n",
    "                                     scoring = 'neg_mean_squared_error', \n",
    "                                     cv = 5, n_jobs = 6, n_iter = 10, verbose=1)\n",
    "#fit search model\n",
    "glm_search_results = glm_kt_model_search.fit(X_train, y_train)\n",
    "\n",
    "#set params based on search\n",
    "glm_kt_model.set_params(**glm_search_results.best_params_)\n",
    "\n",
    "#fit model using best params ()\n",
    "glm_kt_model.fit(X_train, y_train)\n",
    "\n",
    "#predict and show error\n",
    "y_pred = glm_kt_model.predict(X_test)\n",
    "msec = mse_EucDistance(y_test,y_pred)\n",
    "glm_search_results_pd = pd.DataFrame(glm_search_results.cv_results_)\n",
    "\n",
    "print('-----------------------------------------------------------------------------------------------')\n",
    "print('Mean summed/mean physical distance error for (x,y) location estimation: {:3.1f} / {:3.1f}: meters'.format(mse,msec))\n",
    "print(\"Optimized parameters in rank order (based on tested values):\")\n",
    "print(glm_search_results_pd.sort_values(by='rank_test_score').filter(items=['param_kernel_s0', 'param_kernel_s1','param_kernel_s2','param_lambdau','param_skl_kernel','mean_test_score']).head())\n",
    "print('-----------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#hide\n",
    "#sklearn 0.23.1 and skopt 0.74... broken dependency between the two... wait 'til one or other is upgraded\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "opt_params = {\n",
    "    'skl_model__alpha': (1e-8, 1e+0, 'log-uniform'),\n",
    "    'kernel_s0': (1e-6, 1e+1, 'log-uniform'),'kernel_s1': (1e-6, 1e+1, 'log-uniform'), \n",
    "    'kernel_s2':(1e-6, 1e+1, 'log-uniform'),\n",
    "    'skl_kernel': ['laplacian', 'rbf'],  # categorical parameter\n",
    "}\n",
    "\n",
    "opt_skl_kt = BayesSearchCV(skl_kt_model, opt_params, n_iter=32, cv=5, \n",
    "                           n_jobs = 1, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "opt_skl_kt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[^1]: A. A. M. Saleh and R. Valenzuela, \"A Statistical Model for Indoor Multipath Propagation,\" in IEEE Journal on Selected Areas in Communications, vol. 5, no. 2, pp. 128-137, February 1987, doi: 10.1109/JSAC.1987.1146527.\n",
    "\n",
    "[^2]: Theodore S. Rappaport's Wireless Communications: Principles and Practice by IEEE Press, Inc. Prentic Hall ISBN: 0-7803-1167-1. Chapters 3 and 4\n",
    "\n",
    "[^3]: Q. H. Spencer, B. D. Jeffs, M. A. Jensen and A. L. Swindlehurst, \"Modeling the statistical time and angle of arrival characteristics of an indoor multipath channel,\" in IEEE Journal on Selected Areas in Communications, vol. 18, no. 3, pp. 347-360, March 2000, doi: 10.1109/49.840194."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
